= What's the matter with science?

title: What's the matter with science?
Slug: Why can't we trust the results we seem to publish?
Date: 2015-06-19 11:00
Tags: statistics, replication
Author: Krishna Bhogaonker
Email:  krishnablog@gmail.com
Github: 00krishna
Site: http://blog.krishnab.com
Twitter: 00_krishna
Linkedin: sbhogaonker
Category: replication

What is the matter with science? While most of the recent media attention has focused on high profile scientific retractions, such as the Lacour and Green study, there is an even bigger problem in science. While data falsification and the drama of obfuscation do make for good editorial writing, it is much more difficult to make a story out of well-intentioned scientists producing research that follows established scientific standards, but that still turns out to be invalid. Add to this the fact that the invalidity of these findings are only discovered years after the study has been published.

The story that is not getting attention in the media is just how pervasive the failure of replication has been in so many different areas of science. I suppose it all began in biology when Begley and Ellis of Amgen published their attempts to validate and replicate 55 landmark studies in biology and particularly oncology. When they could not replicate a study in their own laboratories, they went back to the original lab that published the paper and asked them to replicate the study. Given this methodology, the authors could only replicate 6 out of 53 papers. That is an 89% failure rate.

Now these were not scientists who were delibrately trying to fabricate their data or results. These were scientists who had followed experimental protocols as they has been established by their own discipline. However, even with these standards in place, the results produced should never have been published.

It would be one thing if this problem was limited to only the field of oncology, but this is sadly not the case. In psychiatry there is also an issue with replicating the findings for models of schitzophrenia. Wallock et. al., tested 29 existing PANSS five factor models against a newly dataset of schitzophrenia patients. This time, none of the 29 models met the threshold of statistical significance. While the original author's models had worked against their own samples, none of these models worked on samples outside of their own. This presents a problem because we rely on experimental tests to indicate how a drug or treatment will work on the general public and not only those in their own studies.

So why can't we seem to get results that work. Well let me walk you through the process of data analysis and modeling, since I have been there and done that so many times. There is a moment after all the data has been manually coded and the data has been cleaned and loaded, that the researcher sits down for the moment of truth. First he or she might start with their variable of interest and introduce it into the model equation. Once that is done, the research often adds control variables one at a time, each time watching the variable(s) of interest to see if their coefficients remain above or fall below the threshold of statistical significance. After a bit of fine tuning of the control variables to reduce correlations and such, the scholar hopes that he/she has a statistical analysis that confirms their hypothesis and that plausibly rules out counter explanations.

The problem here is that you have made the cardinal sin of data analysis and contributed to the problem, even if you did not intend to. In the field of statistics there is what is called the multiple comparison's problem. This basically means that the more models you compare, the more likely you are to find a statistically significant result. So by testing different versions of the model, by adding and removing different factors, you are engaging in multiple comparisons. The usual solution to multiple comparisons is to raise the threshold for statistical significance. In other words, the models that you test, the more likely you are to get a statistically significant result, so let's just increase the standard for statistical significance.

This is not a novel approach to the multiple comparisons problem. The Bonferroni correction was first proposed in the 1930s and subsquent development took place in the 1960(1,2,3). The problem is that in many disciplines scholars are not using bonferroni or some better correction method to ensure that their results are genuinely statistically significant. This month the physics world was abuzz with the discussion of 5 sigma. Five sigma represents a result that is five standard deviations from the mean of the null distribution, in other words strong refutation of the null hypothesis. The usual standard is about 2 sigma, so 5 sigma represents about 3 orders of magnitude greater significance.

The discussion of 5 sigma also represents and acknowledgement by the physics community that big discoveries require greater scrutiny and stronger standards of evidence. By apply a 5 sigma standard, the physicists are really saying that they want to make sure the model is true, despite different permutations of the model. Perhaps this is a standard that many other disciplines should adopt as well.

So what is the import of this discussion. There are a number of other articles out there that say exactly what I am saying. But the real question for the researchers is how can he/she go about their research without testing a variety of different models. This seems to defy the logic of the scientific process. Further, imposing a five sigma limit on publishable results would drastically reduce the number of articles submitted to journals, and the number of articles approved for publication. This standard would also mean that a large number of associate professors would fail their tenure reviews because they could not publish enough articles, and it would mean that a large amount of research funds were "wasted" because no five sigma results were found and no publications were produced.

So far the academic community has responded to the problem with deafening silence. While a number of articles and blog posts have been written about this issue, I have yet to see any changes in the training curricula of social science departments, or other science departments. Open access journals have grown in popularity as they often require article submitters to publish their raw data along with the article--so that outside scholars can scrutinize the results.








(1)
Bonferroni, C. E., Teoria statistica delle classi e calcolo delle probabilità, Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 1936
(2)Dunn, Olive Jean (1959). "Estimation of the Medians for Dependent Variables". Annals of Mathematical Statistics 30 (1): 192–197. JSTOR 2237135.
(3)Dunn, Olive Jean (1961). "Multiple Comparisons Among Means" (PDF). Journal of the American Statistical Association 56 (293): 52–64. doi:10.1080/01621459.1961.10482090.
